{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "# !pip3 install spacy \n",
    "# !python -m spacy download ru_core_news_sm\n",
    "import spacy\n",
    "import re\n",
    "from string import punctuation\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "def lemm_phrase(x):\n",
    "    tokens = mystem.lemmatize(x)\n",
    "    tokens = [token for token in tokens[:-1] if (token.strip() not in [\" \", \"\"])]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work with generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/generated_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\"|\\'|\\.|,|\\\\|\\/|-|\\(|\\)|,')\n",
    "\n",
    "df['new_text'] = df['text'].apply(lambda x: re.sub(pattern=pattern, repl='', string=x.lower()).strip())\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(pattern=pattern, repl='', string=x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_text'] = df['new_text'].apply(lambda x: lemm_phrase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('dataset/final_data_lower.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/final_data_lower.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = re.compile(r':')\n",
    "df['text'] = df['text'].apply(lambda x: re.sub(pattern=pattern, repl=' : ', string=x).strip())\n",
    "df['new_text'] = df['new_text'].apply(lambda x: re.sub(pattern=pattern, repl=' : ', string=x).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['new_text'] = df['new_text'].apply(lambda x: ' '.join([i for i in x.split() if i != '']))\n",
    "df['text'] = df['text'].apply(lambda x: ' '.join([i for i in x.split() if i != '']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.ru import Russian\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    'создавать задача',\n",
    "    'создавать заметка',\n",
    "    'создавать задание',\n",
    "    'создавать поручение',\n",
    "    'напоминать',\n",
    "    'запланировать встреча',\n",
    "    'назначать обязанность',\n",
    "    'ставить задача',\n",
    "    'задавать задача',\n",
    "    'давать поручение',\n",
    "    'давать задача',\n",
    "    'добавлять задача',\n",
    "    'поставлять задача'\n",
    "]\n",
    "\n",
    "tags = {\n",
    "    \"Task\": 'task_type',\n",
    "    \"ToDo\": 'todo',\n",
    "    \"Person\": 'person',\n",
    "    \"Time\": 'time',\n",
    "    \"Garbage\": 'garbage'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(data, type):\n",
    "    data = data\n",
    "    patterns = []\n",
    "    for item in data:\n",
    "        pattern = {\"label\": type, \"pattern\": item}\n",
    "        # generate list of patterns\n",
    "        patterns.append(pattern)\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = Russian()\n",
    "ruler = EntityRuler(nlp)\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "def generate_rules(patterns, name):\n",
    "    ruler.add_patterns(patterns)\n",
    "    # nlp.to_disk(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, text):\n",
    "    doc = model(text)\n",
    "    results = []\n",
    "    entities = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        entities.append((ent.start_char, ent.end_char, ent.label_))\n",
    "    if len(entities) > 0:\n",
    "        results = [text, {\"entities\": entities}]\n",
    "        return results\n",
    "\"\"\"\n",
    "TRAIN DATA FOR SPACY = [\n",
    "        (   text, \n",
    "            {\"entities\": \n",
    "                [(start, end, label), ...]\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_type(model, x, arr):\n",
    "    results = test_model(model, x)\n",
    "    if results != None:\n",
    "        arr.append(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение типа задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_patterns = create_training_data(tasks, tags['Task'])\n",
    "generate_rules(patterns=task_patterns, name='task_types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение имен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fem = pd.read_csv('dataset/female_names.csv')\n",
    "# df_male = pd.read_csv('dataset/male_names.csv')\n",
    "# df_names = df_fem.append(df_male)\n",
    "# df_names.to_csv('dataset/names.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_names['name'] = df_names['name'].apply(lambda x: x.lower())\n",
    "# df_names['name'] = df_names['name'].apply(lambda x: lemm_phrase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names = pd.read_csv('dataset/names.csv')\n",
    "person_patterns = create_training_data(list(df_names['name']), tags['Person'])\n",
    "generate_rules(patterns=person_patterns, name='person')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Выделение времени"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = pd.read_csv('dataset/with_months.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_time.to_csv('dataset/time.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_time['time'] = df_time['time'].apply(lambda x: x.lower())\n",
    "# df_time['lower_time'] = df_time['time'].apply(lambda x: lemm_phrase(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_patterns = create_training_data(list(df_time['lower_time']), tags['Time'])\n",
    "generate_rules(patterns=time_patterns, name='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with all entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('final_all_entities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA = []\n",
    "df['new_text'].apply(lambda x: get_train_type(nlp, x, TRAIN_DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.training.example import Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, epochs=30):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank(\"ru\")\n",
    "    if \"ner\" not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe(\"ner\")\n",
    "        nlp.add_pipe(\"ner\", last=True)\n",
    "\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2]) # taking label for ex. person\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n",
    "\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itr in range(epochs):\n",
    "            print(\"Start Epoch - \" + str(itr))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for batch in spacy.util.minibatch(TRAIN_DATA, size=2):\n",
    "                for text, annotations in batch:\n",
    "                    doc = nlp.make_doc(text)\n",
    "                    example = Example.from_dict(doc, annotations)\n",
    "                    nlp.update(\n",
    "                        [example],\n",
    "                        drop=0.2,\n",
    "                        sgd=optimizer,\n",
    "                        losses=losses\n",
    "                    )\n",
    "            print(losses)\n",
    "    return nlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = train(TRAIN_DATA)\n",
    "nlp.to_disk('final_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use wihtout final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(phrase):\n",
    "    pattern = re.compile(r'\"|\\'|\\.|,|\\\\|\\/|-|\\(|\\)|,')\n",
    "    phrase = re.sub(pattern=pattern, repl='', string=phrase.lower()).strip()\n",
    "    final = lemm_phrase(phrase)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp.to_disk('result_final_all')\n",
    "nlp = spacy.load('final_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод результата к красивом виде\n",
    "def get_results(text):\n",
    "    result = []\n",
    "    idx = []\n",
    "\n",
    "    # подготовка фразы\n",
    "    cleaned = clean_str(text)\n",
    "    tmp = list(zip(text.split(), cleaned.replace(' : ', ':').split()))\n",
    "    doc = nlp(cleaned)\n",
    "    for ent in doc.ents:\n",
    "        words = cleaned[ent.start_char:ent.end_char].split()\n",
    "        res = []\n",
    "        for w in words:\n",
    "            for t, i in enumerate(tmp):\n",
    "                if w in i[1]:\n",
    "                    idx.append(t)\n",
    "                    res.append(i[0])\n",
    "        result.append((' '.join(res), ent.label_))\n",
    "\n",
    "    # получаем \"название для задачи\" - то, что не разметилось = само задание\n",
    "    task = [k[0] for t, k in enumerate(tmp) if t not in idx]\n",
    "\n",
    "    # объединяем выделенные объекты\n",
    "    result = sorted(result, key=lambda x: x[1])\n",
    "    final = {}\n",
    "    key = None\n",
    "    for i in result:\n",
    "        if i[1] != key:\n",
    "            key = i[1]\n",
    "            final[key] = i[0]\n",
    "        else:\n",
    "            final[key] = final[key] + ', ' + i[0]\n",
    "    task = ' '.join(task)\n",
    "    return final, task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'person': 'Александра, Дмитрия', 'task_type': 'Поставь задачу:', 'time': '7, вечера'}, 'приготовить ужин в на и')\n",
      "({'task_type': 'Напомни', 'time': '19, декабря'}, 'выгулять собаку')\n",
      "({'person': 'Екатерину', 'task_type': 'Создай задачу', 'time': '5, часов'}, 'на провести планирование в')\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    'Поставь задачу: приготовить ужин в 7 вечера на Александра и Дмитрия',\n",
    "    'Напомни выгулять собаку 19 декабря',\n",
    "    'Создай задачу на Екатерину провести планирование в 5 часов'\n",
    "    ]\n",
    "\n",
    "for text in texts:\n",
    "    print(get_results(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
